# -*- coding: utf-8 -*-
"""최종본의 사본의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q6N_7gSKnHh_lNg1Tv5POI9vn6TKQzNs
"""

import os
import pathlib

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import models
from IPython import display

#웹에서 녹음
import os
from io import BytesIO
from base64 import b64decode
from google.colab import output
from IPython.display import Javascript
from pprint import pprint
import scipy.io.wavfile
import numpy

!pip install google-cloud-speech
!pip install google.api_core
!pip install google-cloud-vision

# for speechbrain
!pip install -qq torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 torchtext==0.12.0
!pip install -qq speechbrain==0.5.12

# pyannote.audio
!pip install -qq pyannote.audio

# for visualization purposes
!pip install -qq ipython==7.34.0

import io
import os
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = '/content/drive/MyDrive/speakers/noble-triode-366808-8c56383c9a63.json'

# [START speech_quickstart]
# Imports the Google Cloud client library
# [START speech_python_migration_imports]
from google.cloud import speech
import librosa  
# [END speech_python_migration_imports]
def stt(output_file_name="test.wav"):
    # Instantiates a client
    # [START speech_python_migration_client]
    client = speech.SpeechClient()
    # [END speech_python_migration_client]

    # The name of the audio file to transcribe
    file_name = os.path.join(os.path.dirname('.'), ".", output_file_name)



    # Loads the audio into memory
    with io.open(file_name, "rb") as audio_file:
        content = audio_file.read()
        audio = speech.RecognitionAudio(content=content)

    #language code:https://cloud.google.com/speech-to-text/docs/languages
    config = speech.RecognitionConfig(
        encoding = speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED,
        sample_rate_hertz=48000,
        language_code="ko-KR",
    )

    # Detects speech in the audio file
    response = client.recognize(config=config, audio=audio)

    for result in response.results:
        print("{}".format(result.alternatives[0].transcript),end=" ")

    return result.alternatives[0].transcript;
    # [END speech_quickstart]

# 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
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

DATASET_PATH='/content/drive/MyDrive/speakers'
wavFileName='/content/drive/MyDrive/자연1.wav'
data_dir=pathlib.Path(DATASET_PATH)
if not data_dir.exists():
  print("존재하지않음")

from huggingface_hub import notebook_login
notebook_login()

from google.colab import drive
drive.mount('/content/drive')

from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', use_auth_token=True)
diarization = pipeline(wavFileName, num_speakers=3)

#분리 모델
#파일 자르기 및 텐서플로우 변수로 만들기 구현.
import tensorflow as tf
import soundfile as sf
time={}
timelist=[]
speakers=[]
# 현아-2, 수영-1, 혁수-0
for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}")
    time[turn.start]=turn.end
    timelist.append(turn.start)
    speakers.append(speaker)

# 파일 초 단위로 끊기
from pyannote.audio import Audio 
from pyannote.core import Segment, notebook
from IPython.display import Audio as IPythonAudio
num=0
for i in timelist :
  EXCERPT = Segment(i, time[i])
  waveform, sr = Audio().crop(wavFileName, EXCERPT)
  a=IPythonAudio(waveform.flatten(), rate=sr)
  with open('/content/drive/MyDrive/final{}.wav'.format(num), 'wb') as f:
    f.write(a.data)
  num+=1
  print(i)

def get_spectrogram(waveform):
  #파형의 길이가 같아야 스펙트로그램으로 변환할 때 결과가 비슷한 차원을 갖게된다.
  #1초보다 짧은 오디오 클립을 단순히 제로 패딩하여 수행할 수 있다.
  #tf.signal.stft를 호출할 때 생성된 스펙토그램 "이미지"가 거의 정사각형이 되도록 frame_length및
  #frame_step 매개변수를 선택
  #stft는 크기와 위상을 나타내는 복소수 배열을 생성한다. tf.abs의 출력에
  #tf.signal.stft를 적용하여 파생할 수 있는 크기만 사용한다.
  input_len=16000
  waveform = waveform[:input_len]
  zero_padding = tf.zeros(
      [16000] - tf.shape(waveform),
      dtype=tf.float32)
  
#waveform을 텐서플로우의 데이터타입 float32로 바꾼다.
  waveform = tf.cast(waveform,dtype=tf.float32)
# zero padding을 연결하여 모든 오디오를 같은 길이의 클립이 되도록한다.
  equal_length = tf.concat([waveform, zero_padding],0)
  #waveform을 STFT를 통해 스펙트로그램으로 바꾼다.
  spectrogram = tf.signal.stft(
      equal_length, frame_length=255, frame_step=128
  ) #frame_length랑 frame_step은 바뀔 수도 있는 부분, 이 부분은 정사각형이 되도록 구현
  spectrogram = tf.abs(spectrogram)
  #파생할 수 있는 크기만 사용
  #스펙트로그램을 사용할 수 있도록 '채널' 치수를 추가한다. -? tf.newaxis를 사용한이유?
  #합성곱 레이어가 있는 이미지와 같은 입력 데이터를 예상,,, 
  #shape('일괄 크기','높이','너비',채널)
  spectrogram = spectrogram[...,tf.newaxis]
  return spectrogram

#시간에 따른 예제의 파형과 해당 스펙트로그램
def plot_spectrogram(spectrogram,ax):
  if len(spectrogram.shape)>2:
    assert len(spectrogram.shape) ==3
    spectrogram = np.squeeze(spectrogram,axis=-1)
    #주파수를 log scale과 transpose로 바꾼다. 시간이 x 중심선으로 표현하기 위해서
    #zero 로그를 피하기 위해 입실론을 추가한다.
    log_spec = np.log(spectrogram.T+np.finfo(float).eps)
    height=log_spec.shape[0]
    width=log_spec.shape[1]

    X= np.linspace(0,np.size(spectrogram),num=width,dtype=int)
    Y=range(height)
    ax.pcolormesh(X,Y,log_spec)

from keras.models import load_model
new_model = load_model('/content/drive/MyDrive/speakers/ISpeacker5.h5')

def get_label(file_path):
  parts = tf.strings.split(
      input=file_path,
      sep=os.path.sep)
  # Note: You'll use indexing here instead of tuple unpacking to enable this
  # to work in a TensorFlow graph.
  return parts[-2]

def get_waveform_and_label(file_path):
  label = get_label(file_path)
  audio_binary = tf.io.read_file(file_path)
  waveform = decode_audio(audio_binary)
  return waveform, label
  
def get_spectrogram_and_label_id(audio,label):
  spectrogram = get_spectrogram(audio)
  label_id=tf.argmax(label==commands)
  return spectrogram,label_id

def preprocess_dataset(files):
  files_ds = tf.data.Dataset.from_tensor_slices(files)
  output_ds = files_ds.map(
      map_func=get_waveform_and_label,
      num_parallel_calls=AUTOTUNE)
  output_ds = output_ds.map(
      map_func=get_spectrogram_and_label_id,
      num_parallel_calls=AUTOTUNE)
  return output_ds

AUTOTUNE=tf.data.AUTOTUNE
def decode_audio(audio_binary):
  # Decode WAV-encoded audio files to `float32` tensors, normalized
  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.
  audio, _ = tf.audio.decode_wav(contents=audio_binary)
  # Since all the data is single channel (mono), drop the `channels`
  # axis from the array.
  return tf.squeeze(audio, axis=-1)
  #각 파일의 상위 디렉토리를 사용하여 레이블을 생성하는 함수 정의

commands = np.array(tf.io.gfile.listdir(str(data_dir)))
commands = commands[commands != 'README.md']
commands = commands[commands != 'wavData']
commands = commands[commands != 'ISpeacker2.h5']
commands = commands[commands != 'ISpeacker.h5']
commands = commands[commands != 'ISpeacker3.h5']
commands = commands[commands != 'ISpeacker4.h5']
#commands = commands[commands != 'suyoung']
#commands = commands[commands != 'hyeoksu']
commands = commands[commands != '.ipynb_checkpoints']
commands = commands[commands != 'noble-triode-366808-8c56383c9a63.json']
commands = commands[commands != 'ppzZqYxGkESMdA5Az']

print('commands:', commands)

def getResult(name):
  test_ds=preprocess_dataset([str(name)])
  for waveform,label in test_ds.batch(1):
    predict=new_model.predict(waveform)
    print(commands[np.argmax(predict[0])],end=": ")
    print(stt(name))

for i in range(4):
  path="/content/drive/MyDrive/final"+str(i)+".wav"
  getResult(path)