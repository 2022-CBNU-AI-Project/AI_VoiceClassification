# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jqGlfzvL31gf81BUJIpZh-Met9sO7WaU
"""

import os
import pathlib

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import models
from IPython import display

from google.colab import drive
drive.mount('/content/drive')

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

DATASET_PATH='/content/drive/MyDrive/speakers'
data_dir=pathlib.Path(DATASET_PATH)
if not data_dir.exists():
  print("존재하지않음")

def decode_audio(audio_binary):
  # Decode WAV-encoded audio files to `float32` tensors, normalized
  # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.
  audio, sr = tf.audio.decode_wav(contents=audio_binary)
  print(sr)
  # Since all the data is single channel (mono), drop the `channels`
  # axis from the array.
  return tf.squeeze(audio, axis=-1)
  #각 파일의 상위 디렉토리를 사용하여 레이블을 생성하는 함수 정의

def get_label(file_path):
  parts = tf.strings.split(
      input=file_path,
      sep=os.path.sep)
  # Note: You'll use indexing here instead of tuple unpacking to enable this
  # to work in a TensorFlow graph.
  return parts[-2]

def get_waveform_and_label(file_path):
  label = get_label(file_path)
  print(label)
  audio_binary = tf.io.read_file(file_path)
  waveform = decode_audio(audio_binary)
  return waveform, label

#라벨을 가져오는 부분
commands = np.array(tf.io.gfile.listdir(str(data_dir)))
commands = commands[commands != 'README.md']
commands = commands[commands != 'noble-triode-366808-8c56383c9a63.json']
commands = commands[commands != '.ipynb_checkpoints']
commands = commands[commands != 'ISpeacker.h5']
commands = commands[commands != 'ISpeacker2.h5']
commands = commands[commands != 'ISpeacker3.h5']
commands = commands[commands != 'ISpeacker4.h5']
commands = commands[commands != 'wavData']
commands = commands[commands != 'ppzZqYxGkESMdA5Az']
print('commands:', commands)

filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')
filenames = tf.random.shuffle(filenames)
num_samples = len(filenames)
print('Number of total examples:', num_samples)
print('Number of examples per label:',
      len(tf.io.gfile.listdir(str(data_dir/commands[0]))))
print('Example file tensor:', filenames[0])

#train_files = filenames[:18951] #왜 예제에서 이렇게 나눠서 하는이유가 라벨을 정확하게 분류할려면 이렇게 해야하는거같아ㅛ
#val_files = filenames[18951:18951 + 5880] #아 근데 이게 랜덤적으로 뽑는게 아니라 반복문으로 0.2씩 뽑았잖아요
#이렇게 되니까 라벨이 0~30개가 일치하고 40~50일치하고 이런문제가 발생 하는거 같아요 (근데 전체 파일에서 랜덤이 아니고 같은 라벨에서 
#test_files = filenames[-5880:]

#print('Training set size', len(train_files))
#print('Validation set size', len(val_files))
#print('Test set size', len(test_files))

#for folder in commands:
#  filenames = tf.io.gfile.glob(str(data_dir) + f'/{folder}/*.wav')
#  filenames = tf.random.shuffle(filenames)
#  print(f"{folder}: {len(filenames)}")
#  testlen=int(len(filenames)*test_rate)
#  print(testlen)
#  train_files+=list(filenames[:-testlen])
#  test_files+=list(filenames[-testlen:])


#print('Number of total train data:', len(train_files))
#print('Number of total test data:', len(test_files))
#print('Example file tensor:', train_files[0])

#val_len=int(len(train_files)*test_rate)
#train_files = tf.random.shuffle(train_files)
#train_files=train_files[:-val_len]
#val_files=train_files[-val_len:]
#test_files = tf.random.shuffle(test_files)

#파일 이름 가져오는 부분
test_rate=0.2
train_files = []
test_files = [] #tf.io.gfile.glob(str(data_dir) + '/testData/*/*.wav')


for folder in commands:
  filenames = tf.io.gfile.glob(str(data_dir) + f'/{folder}/*.wav')
  filenames = tf.random.shuffle(filenames)
  print(f"{folder}: {len(filenames)}")
  testlen=int(len(filenames)*test_rate)
  
  if testlen == 0:
      testlen=1 
  print(testlen)
  train_files+=list(filenames[:-testlen])
  test_files+=list(filenames[-testlen:])


print('Number of total train data:', len(train_files))
print('Number of total test data:', len(test_files))
print('Example file tensor:', train_files[0])

val_len=int(len(train_files)*test_rate)
train_files = tf.random.shuffle(train_files)
train_files=train_files[:-val_len]
val_files=train_files[-val_len:]

print('Total size', len(train_files)+len(val_files)+len(test_files))
print('Training set size', len(train_files))
print('Validation set size', len(val_files))
print('Test set size', len(test_files))

#train_files = filenames[:23569]
#val_files = filenames[23569:23569+6000]
#test_files = filenames[-6000:]
#print('Training set size', len(train_files))
#print('Validation set size', len(val_files))
#print('Test set size', len(test_files))

AUTOTUNE=tf.data.AUTOTUNE

files_ds=tf.data.Dataset.from_tensor_slices(train_files)

waveform_ds=files_ds.map(
    map_func=get_waveform_and_label,
    num_parallel_calls=AUTOTUNE)

rows = 3
cols = 3
n = rows * cols
fig, axes = plt.subplots(rows, cols, figsize=(10, 12))

for i, (audio, label) in enumerate(waveform_ds.take(n)):
  r = i // cols
  c = i % cols
  ax = axes[r][c]
  ax.plot(audio.numpy())
  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))
  label = label.numpy().decode('utf-8')
  ax.set_title(label)

plt.show()

def get_spectrogram(waveform):
  #파형의 길이가 같아야 스펙트로그램으로 변환할 때 결과가 비슷한 차원을 갖게된다.
  #1초보다 짧은 오디오 클립을 단순히 제로 패딩하여 수행할 수 있다.
  #tf.signal.stft를 호출할 때 생성된 스펙토그램 "이미지"가 거의 정사각형이 되도록 frame_length및
  #frame_step 매개변수를 선택
  #stft는 크기와 위상을 나타내는 복소수 배열을 생성한다. tf.abs의 출력에
  #tf.signal.stft를 적용하여 파생할 수 있는 크기만 사용한다.
  input_len=16000
  waveform = waveform[:input_len]
  zero_padding = tf.zeros(
      [16000] - tf.shape(waveform),
      dtype=tf.float32)
  
#waveform을 텐서플로우의 데이터타입 float32로 바꾼다.
  waveform = tf.cast(waveform,dtype=tf.float32)
# zero padding을 연결하여 모든 오디오를 같은 길이의 클립이 되도록한다.
  equal_length = tf.concat([waveform, zero_padding],0)
  #waveform을 STFT를 통해 스펙트로그램으로 바꾼다.
  spectrogram = tf.signal.stft(
      equal_length, frame_length=255, frame_step=128
  ) #frame_length랑 frame_step은 바뀔 수도 있는 부분, 이 부분은 정사각형이 되도록 구현

  spectrogram = tf.abs(spectrogram)
  #파생할 수 있는 크기만 사용
  #스펙트로그램을 사용할 수 있도록 '채널' 치수를 추가한다. -? tf.newaxis를 사용한이유?
  #합성곱 레이어가 있는 이미지와 같은 입력 데이터를 예상,,, 
  #shape('일괄 크기','높이','너비',채널)
  spectrogram = spectrogram[...,tf.newaxis]
  mfccs = tf.signal.mfccs_from_log_mel_spectrograms(
  spectrogram)[..., tf.newaxis]
  return spectrogram

for waveform, label in waveform_ds.take(1):
  label = label.numpy().decode('utf-8')
  print(tf.shape(waveform)) 
  spectrogram = get_spectrogram(waveform)

print('Label:', label)
print('Waveform shape:', waveform.shape)
print('Spectrogram shape:', spectrogram.shape)
print('Audio playback')
display.display(display.Audio(waveform, rate=16000))

import sklearn
import librosa
import librosa.display
import matplotlib.pyplot

rows = 3
cols = 3
n = rows*cols
fig,axes = plt.subplots(rows,cols,figsize=(10,10))

for i, (spectrogram, label) in enumerate(waveform_ds.take(300)):
  if label=="suyoung" or label == "hyeona" or label == "hyeoksu" :
    mfccs = librosa.feature.mfcc(spectrogram.numpy(), sr=16000)
    mfccs = sklearn.preprocessing.minmax_scale(mfccs, axis=1) 

    librosa.display.specshow(mfccs, sr=16000, x_axis='time')
    plt.colorbar()
    plt.title(label.numpy())
    plt.axis('off')
    plt.show()

#시간에 따른 예제의 파형과 해당 스펙트로그램
def plot_spectrogram(spectrogram,ax):
  if len(spectrogram.shape)>2:
    assert len(spectrogram.shape) ==3
    spectrogram = np.squeeze(spectrogram,axis=-1)
    #주파수를 log scale과 transpose로 바꾼다. 시간이 x 중심선으로 표현하기 위해서
    #zero 로그를 피하기 위해 입실론을 추가한다.
    log_spec = np.log(spectrogram.T+np.finfo(float).eps)
    height=log_spec.shape[0]
    width=log_spec.shape[1]

    X= np.linspace(0,np.size(spectrogram),num=width,dtype=int)
    Y=range(height)
    ax.pcolormesh(X,Y,log_spec)

#시간에 따른 예제의 파형과 스펙트로그램(시간에 따른 주파수)를 플로팅한다.

fig,axes = plt.subplots(2, figsize=(12,8))
timescale = np.arange(waveform.shape[0])
axes[0].plot(timescale,waveform.numpy())
axes[0].set_title('Waveform')
axes[0].set_xlim([0,16000])

plot_spectrogram(spectrogram.numpy(),axes[1])
axes[1].set_title('Spectrogram')
plt.show()

#파형 데이터 세트를 스펙트로그램으로 변환하고 해당 레이블을 정수 id로 변환하는 함수 정의

def get_spectrogram_and_label_id(audio,label):
  spectrogram = get_spectrogram(audio)
  label_id=tf.argmax(label==commands)
  print(commands)
  print(label)
  return spectrogram,label_id


spectrogram_ds = waveform_ds.map(
    map_func=get_spectrogram_and_label_id,
    num_parallel_calls=AUTOTUNE)
print([16000] - tf.shape(waveform))

rows = 3
cols = 3
n = rows*cols
fig,axes = plt.subplots(rows,cols,figsize=(10,10))

for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):
  r = i // cols
  c = i % cols
  ax = axes[r][c]
  plot_spectrogram(spectrogram.numpy(), ax)
  ax.set_title(commands[label_id.numpy()])
  ax.axis('off')

plt.show()

import sklearn
import librosa
title = ['MFCCs : F1_high','MFCCs : F1', 
        'MFCCs : F2','MFCCs : F3','MFCCs : M1',
        'MFCCs : M2','MFCCs : M2_low']


for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):
    mfccs = librosa.feature.mfcc(spectrogram, sr=16000)
    print('mfccs shape:', mfccs.shape)

    mfccs = sklearn.preprocessing.minmax_scale(mfccs, axis=1) 
    print('mean: %.2f' % mfccs.mean())
    print('var: %.2f' % mfccs.var())

    plt.figure(figsize=(16, 6))
    librosa.display.specshow(mfccs, sr=16000, x_axis='time')
    plt.title(title[i], fontsize=20)
    plt.colorbar()
    plt.show()

def preprocess_dataset(files):
  files_ds = tf.data.Dataset.from_tensor_slices(files)
  output_ds = files_ds.map(
      map_func=get_waveform_and_label,
      num_parallel_calls=AUTOTUNE)
  output_ds = output_ds.map(
      map_func=get_spectrogram_and_label_id,
      num_parallel_calls=AUTOTUNE)
  return output_ds
#데이터셋 전처리용 함수

#train_ds=spectrogram_ds
#test_filenames = tf.io.gfile.glob(str(test_dir) + '/*/*')
#test_filenames = tf.random.shuffle(test_filenames)
#test_files=test_filenames[:len(test_filenames)]
#val_ds=preprocess_dataset(val_files)
#test_ds=preprocess_dataset(test_files)

#모델 훈련을 위한 훈련 및 검증 세트를 일괄 처리

#batch_size=64
#train_ds = train_ds.batch(batch_size)
#val_ds = val_ds.batch(batch_size)
#test_ds=test_ds.batch(batch_size)

#dataset.cache 및 dataset.prfetch 작업을 추가하여 모델을 훈련하는 동안 읽기 지연시간을 줄인다.

#train_ds = train_ds.cache().prefetch(AUTOTUNE)
#val_ds=val_ds.cache().prefetch(AUTOTUNE)
#test_ds=test_ds.cache().prefetch(AUTOTUNE) 
train_ds = spectrogram_ds
val_ds = preprocess_dataset(val_files)
test_ds = preprocess_dataset(test_files)

batch_size = 64
train_ds = train_ds.batch(batch_size)
val_ds = val_ds.batch(batch_size)

train_ds = train_ds.cache().prefetch(AUTOTUNE)
val_ds = val_ds.cache().prefetch(AUTOTUNE)

#오디오 파일을 스펙트로그램 이미지로 변환했으므로 간단한 CNN을 사용

#tf.keras.Sequential 모델은 Keras전처리 레이어 사용
#tf.keras.layers.Resizing 모델이 더 빠르게 학습할 수 있도록 입력을 다운 샘플링
#tf.keras.layers.Normalization 평균과 표준편차를 기반으로 이미지의 각 픽셀을 정규화

#tf.keras.layes.Normalization 레이어를 생성한다.
#스펙트로그램에 레이어의 상태를 맞춘다.(Normalization.adapt를 가진)

for spectrogram, _ in spectrogram_ds.take(1):
  input_shape = spectrogram.shape
print('Input shape:', input_shape)
num_labels = len(commands)

# Instantiate the `tf.keras.layers.Normalization` layer.
norm_layer = layers.Normalization()
# Fit the state of the layer to the spectrograms
# with `Normalization.adapt`.
norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))

model = models.Sequential([
    layers.Input(shape=input_shape),
    # Downsample the input.
    layers.Resizing(32, 32),
    # Normalize.
    norm_layer,
    layers.Conv2D(32, 3, activation='relu'),
    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_labels),
])

model.summary()

#Adam 옵티마이저와 교차 엔트로피 손실을 사용하여 Keras 모델을 구성

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
)

EPOCHS = 500 #20 40 60 80 100
history = model.fit(
      train_ds,
      validation_data=val_ds,
      epochs=EPOCHS,
      #callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),
)

fig,loss_ax = plt.subplots()
acc_ax=loss_ax.twinx()

loss_ax.plot(history.history['loss'], 'y', label='train loss')
loss_ax.plot(history.history['val_loss'], 'r', label='val loss')

acc_ax.plot(history.history['accuracy'], 'b', label='train acc')
acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuray')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='lower left')

plt.show()

test_audio = []
test_labels = []

for audio, label in test_ds:
  test_audio.append(audio.numpy())
  test_labels.append(label.numpy())

test_audio = np.array(test_audio)
test_labels = np.array(test_labels)

y_pred = np.argmax(model.predict(test_audio), axis=1)
y_true = test_labels

test_acc = sum(y_pred == y_true) / len(y_true)
print(f'Test set accuracy: {test_acc:.0%}')

model.save('/content/drive/MyDrive/speakers/ISpeacker5.h5')
model.save_weights('/content/drive/MyDrive/speakers')
checkpoint_path = "/content/drive/MyDrive/speakers/cp5.ckpt"

cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                 save_weights_only=True,
												                         verbose=1)


from google.colab import files
files.download('/content/drive/MyDrive/speakers/ISpeacker5.h5')
#test_loss, test_acc = new_model.evaluate(test_ds, val_ds, verbose=2)

"""여기부터 실행하면됩니다!"""

from keras.models import load_model
new_model = load_model('/content/drive/MyDrive/speakers/ISpeacker.h5')

pwd

# 새로운 콜백으로 모델 훈련하기
new_model.fit(train_ds,
      validation_data=val_ds,
      epochs=10
      )  # 콜백을 훈련에 전달합니다

# 옵티마이저의 상태를 저장하는 것과 관련되어 경고가 발생할 수 있습니다.
# 이 경고는 (그리고 이 노트북의 다른 비슷한 경고는) 이전 사용 방식을 권장하지 않기 위함이며 무시해도 좋습니다.

import matplotlib.pyplot as plt

for spectrogram, label in waveform_ds:
  predict=new_model.evaluate(spectrogram)
  print(predict)

test_audio = []
test_labels = []

for audio, label in test_ds:
  test_audio.append(audio.numpy())
  test_labels.append(label.numpy())

test_audio = np.array(test_audio)
test_labels = np.array(test_labels)

y_pred = np.argmax(new_model.predict(test_audio), axis=1)
y_true = test_labels

test_acc = sum(y_pred == y_true) / len(y_true)
print(f'Test set accuracy: {test_acc:.0%}')